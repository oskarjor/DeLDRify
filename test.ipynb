{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeLDRify - ESRGAN applied to single-track LDR to HDR image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and initalize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"RGBE\": False,\n",
    "            \"in_nc\":3,\n",
    "            \"nf\":64,\n",
    "            \"nb\":8,\n",
    "            \"gc\":32,\n",
    "            \"out_channels\":4,\n",
    "            \"channels\":16,\n",
    "            \"nb_images\":40,\n",
    "            \"batch_size\":4,\n",
    "            \"epochs\":100,\n",
    "            \"loss_scaling_factor\":1e-3,\n",
    "        }\n",
    "\n",
    "if config[\"RGBE\"]:\n",
    "    config[\"in_channels\"] = 4\n",
    "    config[\"out_nc\"] = 4\n",
    "else:\n",
    "    config[\"in_channels\"] = 3\n",
    "    config[\"out_nc\"] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RRDBNet\n",
    "from discriminator import DiscriminatorForVGG\n",
    "\n",
    "G = RRDBNet(in_nc=config[\"in_nc\"], out_nc=config[\"out_nc\"], nf=config[\"nf\"], nb=config[\"nb\"], gc=config[\"gc\"])\n",
    "D = DiscriminatorForVGG(in_channels=config[\"in_channels\"], out_channels=config[\"out_channels\"], channels=config[\"channels\"])\n",
    "\n",
    "G.to(device=device)\n",
    "D.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of param (G):\", sum(p.numel() for p in G.parameters()) / 1_000_000, \"M\")\n",
    "print(\"Number of param (D):\", sum(p.numel() for p in D.parameters()) / 1_000_000, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Resize((128, 128), antialias=None), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import PairWiseImages, PairWiseImagesRGBE\n",
    "\n",
    "### RGBE format\n",
    "if config[\"RGBE\"]:\n",
    "    pair = PairWiseImagesRGBE(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                        \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                        transform=train_transform, device=device)\n",
    "    \n",
    "### Original dataset RGB format\n",
    "else:\n",
    "    pair = PairWiseImages(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                        \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                        transform=train_transform, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "indices = torch.arange(config[\"nb_images\"])\n",
    "pair_40 = Subset(pair, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "length = len(pair_40)\n",
    "test_length = int(0.2 * length)\n",
    "\n",
    "train, valid = torch.utils.data.random_split(pair_40, [length - test_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_ckp(checkpoint, checkpoint_dir, suffix=\"\"):\n",
    "    old_path = f\"{checkpoint_dir}/checkpoint_{suffix}_{checkpoint['epoch']-1}.pt\"\n",
    "    if os.path.exists(old_path):\n",
    "        os.remove(old_path)\n",
    "    f_path = f\"{checkpoint_dir}/checkpoint_{suffix}_{checkpoint['epoch']}.pt\"\n",
    "    torch.save(checkpoint, f_path)\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "                    project=\"DeLDRify\",\n",
    "                    config=config,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_pixel = torch.nn.L1Loss().to(device)\n",
    "criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_dir = './cache-' + run.name\n",
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(results_dir + '/metadata.json', 'w') as fp:\n",
    "    json.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "retrain = False\n",
    "d_path = \"cache-peachy-dawn-11/checkpoint_discriminator_20.pt\"\n",
    "g_path = \"cache-peachy-dawn-11/checkpoint_generator_20.pt\"\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "\n",
    "loss_scaling_factor = config[\"loss_scaling_factor\"]\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "\n",
    "if retrain:\n",
    "    G, optimizer_G, start_epoch = load_ckp(g_path, G, optimizer_G)\n",
    "    D, optimizer_D, start_epoch = load_ckp(d_path, D, optimizer_D)\n",
    "    epochs = epochs + start_epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(start_epoch,epochs)):\n",
    "    total_loss_G = 0\n",
    "    total_loss_D = 0\n",
    "    total_loss_D_real = 0\n",
    "    total_loss_D_fake = 0\n",
    "\n",
    "    for ldr, hdr in tqdm(train_dataloader, leave=False):\n",
    "\n",
    "        D_output_shape = D.out_channels\n",
    "\n",
    "        valid = torch.tensor(np.ones((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "        fake = torch.tensor(np.zeros((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_hdr = G(ldr)\n",
    "\n",
    "        loss_pixel = criterion_pixel(fake_hdr, hdr)\n",
    "\n",
    "        pred_real = D(hdr).detach()\n",
    "        pred_fake = D(fake_hdr)\n",
    "\n",
    "        loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        loss_G = loss_pixel + loss_scaling_factor * loss_GAN\n",
    "        total_loss_G += loss_G.item()\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = D(hdr)\n",
    "        pred_fake = D(fake_hdr.detach())\n",
    "\n",
    "        loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        total_loss_D_real += loss_real.item()\n",
    "        total_loss_D_fake += loss_fake.item()\n",
    "        total_loss_D += loss_D.item()\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    wandb.log({\"loss_G\": total_loss_G / len(train_dataloader), \"loss_D\": total_loss_D / len(train_dataloader), \"loss_D_real\": total_loss_D_real / len(train_dataloader), \"loss_D_fake\": total_loss_D_fake / len(train_dataloader), \"epoch\": epoch+1})\n",
    "\n",
    "\n",
    "    checkpoint_G = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': G.state_dict(),\n",
    "        'optimizer': optimizer_G.state_dict(),\n",
    "    }\n",
    "\n",
    "    checkpoint_D = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': D.state_dict(),\n",
    "        'optimizer': optimizer_D.state_dict(),\n",
    "    }\n",
    "\n",
    "    save_ckp(checkpoint_G,results_dir,\"generator\")\n",
    "    save_ckp(checkpoint_D,results_dir,\"discriminator\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RGBE_to_RGB, preprocess_tensor_to_array\n",
    "from utils import hdr_np_to_img_manual, hdr_np_to_img_tonemap, np_to_img_naive\n",
    "\n",
    "\n",
    "\n",
    "# load data from train dataset onto cpu\n",
    "ldr, hdr = train[0]\n",
    "ldr = preprocess_tensor_to_array(ldr, RGBE=False)\n",
    "hdr = preprocess_tensor_to_array(hdr, RGBE=True)\n",
    "\n",
    "# convert from RGBE to RGB\n",
    "hdr_RGB = RGBE_to_RGB(hdr.copy())\n",
    "\n",
    "# create PIL images (naive approach)\n",
    "ldr_img = np_to_img_naive(ldr)\n",
    "hdr_img = np_to_img_naive(hdr_RGB)\n",
    "\n",
    "# create PIL images (tonemap approach)\n",
    "new_hdr_img = hdr_np_to_img_manual(hdr_RGB)\n",
    "new_hdr2_img = hdr_np_to_img_tonemap(hdr_RGB)\n",
    "\n",
    "# list of images to display\n",
    "images = [ldr_img, hdr_img, new_hdr_img, new_hdr2_img]\n",
    "\n",
    "# display images\n",
    "display(*images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RRDBNet\n",
    "from discriminator import DiscriminatorForVGG\n",
    "import torch\n",
    "\n",
    "# cache to read model weights from\n",
    "target_dir = \"cache-elated-rain-5\"\n",
    "\n",
    "# create models\n",
    "G = RRDBNet(in_nc=3, out_nc=4, nf=64, nb=8, gc=32)\n",
    "D = DiscriminatorForVGG(in_channels=4, out_channels=4, channels=64)\n",
    "\n",
    "# load weights\n",
    "G.load_state_dict(torch.load(f\"{target_dir}/generator_last.pth\"))\n",
    "D.load_state_dict(torch.load(f\"{target_dir}/discriminator_last.pth\"))\n",
    "\n",
    "# move models to device (cpu, cuda or mps)\n",
    "G.to(device=device)\n",
    "D.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a batch from the validation dataset\n",
    "ldr, hdr = next(iter(valid_data_loader))\n",
    "\n",
    "# create a real, generated and random HDR image\n",
    "real_hdr = hdr\n",
    "fake_hdr = G(ldr)\n",
    "random_hdr = torch.rand_like(hdr)\n",
    "\n",
    "# run discriminator on the images\n",
    "print(\"Real HDR images (values should be positive)\")\n",
    "print(D(hdr).mean(0, keepdim=True).detach())\n",
    "print()\n",
    "\n",
    "print(\"Random HDR images (values should be negative)\")\n",
    "print(D(random_hdr).mean(0, keepdim=True).detach())\n",
    "print()\n",
    "\n",
    "print(\"Generated HDR images (values should be positive for a good generator, and negative for a good discriminator)\")\n",
    "print(D(fake_hdr).mean(0, keepdim=True).detach())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_arr = preprocess_tensor_to_array(fake_hdr[0].detach(), RGBE=True)\n",
    "fake_arr_RGB = RGBE_to_RGB(fake_arr)\n",
    "fake_img = hdr_np_to_img_tonemap(fake_arr_RGB)\n",
    "\n",
    "real_arr = preprocess_tensor_to_array(real_hdr[0].detach(), RGBE=True)\n",
    "real_arr_RGB = RGBE_to_RGB(real_arr)\n",
    "real_img = hdr_np_to_img_tonemap(real_arr_RGB)\n",
    "\n",
    "display(fake_img, real_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assert that RGB -> RGBE -> RGB conversion is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from utils import RGBE_to_RGB, RGB_to_RGBE\n",
    "\n",
    "np_hdr = cv.imread(\"LDR-HDR-pair_Dataset-master/HDR/HDR_001.hdr\", flags=cv.IMREAD_ANYDEPTH)\n",
    "print(np_hdr.shape, np_hdr.dtype, np_hdr.max(), np_hdr.min(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_hdr = RGB_to_RGBE(np_hdr)\n",
    "print(converted_hdr.shape, converted_hdr.dtype, converted_hdr.max(), converted_hdr.min(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_hdr_exp = converted_hdr[:, :, 3]\n",
    "converted_hdr_ch = converted_hdr[:, :, :3]\n",
    "print(converted_hdr_exp.max(), converted_hdr_exp.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_converted_hdr = RGBE_to_RGB(converted_hdr)\n",
    "print(double_converted_hdr.shape, double_converted_hdr.dtype, double_converted_hdr.max(), double_converted_hdr.min(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
