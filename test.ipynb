{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeLDRify - ESRGAN applied to single-track LDR to HDR image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and initalize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RRDBNet\n",
    "from discriminator import DiscriminatorForVGG\n",
    "\n",
    "G = RRDBNet(in_nc=3, out_nc=3, nf=32, nb=4, gc=16)\n",
    "D = DiscriminatorForVGG(in_channels=3, out_channels=3, channels=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param (G): 731011\n",
      "Number of param (D): 1061619\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of param (G):\", sum(p.numel() for p in G.parameters()))\n",
    "print(\"Number of param (D):\", sum(p.numel() for p in D.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "class PairWiseImages(Dataset):\n",
    "\n",
    "    def __init__(self, ldr_path, hdr_path, transform=None) -> None:\n",
    "        self.ldr_path = ldr_path\n",
    "        self.hdr_path = hdr_path\n",
    "        self.transform = transform\n",
    "        self.ldr_list = sorted(os.listdir(ldr_path))\n",
    "        self.hdr_list = sorted(os.listdir(hdr_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ldr_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ldr_img_path = os.path.join(self.ldr_path, self.ldr_list[idx])\n",
    "        hdr_img_path = os.path.join(self.hdr_path, self.hdr_list[idx])\n",
    "        ldr_img = cv.imread(ldr_img_path)\n",
    "        ldr_img = ldr_img.astype(np.float32)\n",
    "        ldr_img /= 255.0\n",
    "        hdr_img = cv.imread(hdr_img_path, flags=cv.IMREAD_ANYDEPTH)\n",
    "        hdr_img /= 4.0\n",
    "        if self.transform:\n",
    "            ldr_img = self.transform(ldr_img)\n",
    "            hdr_img = self.transform(hdr_img)\n",
    "        return ldr_img, hdr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Resize((128, 128), antialias=None), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = PairWiseImages(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                      \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                      transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "indices = torch.arange(40)\n",
    "pair_40 = Subset(pair, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "length = len(pair_40)\n",
    "test_length = int(0.2 * length)\n",
    "\n",
    "train, valid = torch.utils.data.random_split(pair_40, [length - test_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moskarjor\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/oskarjor/DeLDRify/wandb/run-20231127_205551-ojav4xbw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oskarjor/DeLDRify/runs/ojav4xbw' target=\"_blank\">faithful-dawn-10</a></strong> to <a href='https://wandb.ai/oskarjor/DeLDRify' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oskarjor/DeLDRify' target=\"_blank\">https://wandb.ai/oskarjor/DeLDRify</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oskarjor/DeLDRify/runs/ojav4xbw' target=\"_blank\">https://wandb.ai/oskarjor/DeLDRify/runs/ojav4xbw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(project=\"DeLDRify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_pixel = torch.nn.L1Loss().to(device)\n",
    "criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cache-2023-11-27-20-55-53-models'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_dir = './cache-' + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-models\")\n",
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e18d43505b42449f70db7a9d2a47b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d0c8624b3f4fa59442a1d0fbaedc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202cb9aaf26b404f9ecd999e7ede1e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "\n",
    "loss_scaling_factor = 1e-3\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    total_loss_G = 0\n",
    "    total_loss_D = 0\n",
    "\n",
    "    for ldr, hdr in tqdm(train_dataloader, leave=False):\n",
    "\n",
    "        D_output_shape = D.out_channels\n",
    "\n",
    "        valid = torch.tensor(np.ones((ldr.size(0), D_output_shape)), requires_grad=False)\n",
    "        fake = torch.tensor(np.zeros((ldr.size(0), D_output_shape)), requires_grad=False)\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_hdr = G(ldr)\n",
    "\n",
    "        loss_pixel = criterion_pixel(fake_hdr, hdr)\n",
    "\n",
    "        pred_real = D(hdr).detach()\n",
    "        pred_fake = D(fake_hdr)\n",
    "\n",
    "        loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        loss_G = loss_pixel + loss_scaling_factor * loss_GAN\n",
    "        total_loss_G += loss_G.item()\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = D(hdr)\n",
    "        pred_fake = D(fake_hdr.detach())\n",
    "\n",
    "        loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        total_loss_D += loss_D.item()\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    wandb.log({\"loss_G\": total_loss_G / len(train_dataloader), \"loss_D\": total_loss_D / len(train_dataloader), \"epoch\": epoch})\n",
    "\n",
    "    torch.save(G.state_dict(), f\"{results_dir}/generator_last.pth\")\n",
    "    torch.save(D.state_dict(), f\"{results_dir}/discriminator_last.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "ldr_img = PIL.Image.fromarray((ldr.permute(1, 2, 0).numpy() * 255).astype(np.uint8))\n",
    "hdr_img = PIL.Image.fromarray((hdr.permute(1, 2, 0).numpy() / 4 * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_hdr = G(ldr.unsqueeze(0)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_hdr_img = PIL.Image.fromarray((fake_hdr[0].permute(1, 2, 0).detach().numpy() / 4 * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_hdr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(fake_hdr.flatten() - hdr.flatten()).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.flatten()[torch.abs(fake_hdr.flatten() - hdr.flatten()).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D(fake_hdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
