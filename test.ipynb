{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeLDRify - ESRGAN applied to single-track LDR to HDR image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and initalize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"RGBE\": False,\n",
    "            \"in_nc\":3,\n",
    "            \"nf\":64,\n",
    "            \"nb\":4,\n",
    "            \"gc\":32,\n",
    "            \"channels\": 4,\n",
    "            \"nb_images\":-1,\n",
    "            \"batch_size\":8,\n",
    "            \"epochs\":400,\n",
    "            \"loss_scaling_factor\":0.01,\n",
    "            \"depth\": 2, \n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "if config[\"RGBE\"]:\n",
    "    config[\"in_channels\"] = 4\n",
    "    config[\"out_nc\"] = 4\n",
    "    config[\"out_channels\"] = 4\n",
    "else:\n",
    "    config[\"in_channels\"] = 3\n",
    "    config[\"out_nc\"] = 3\n",
    "    config[\"out_channels\"] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RRDBNet\n",
    "from discriminator import DiscriminatorForVGG, DiscriminatorForVGG2\n",
    "\n",
    "G = RRDBNet(in_nc=config[\"in_nc\"], out_nc=config[\"out_nc\"], nf=config[\"nf\"], nb=config[\"nb\"], gc=config[\"gc\"])\n",
    "D = DiscriminatorForVGG2(in_channels=config[\"in_channels\"], out_channels=config[\"out_channels\"], channels=config[\"channels\"], depth=config[\"depth\"])\n",
    "\n",
    "G.to(device=device)\n",
    "D.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param (G): 2.918147 M\n",
      "Number of param (D): 0.116299 M\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of param (G):\", sum(p.numel() for p in G.parameters()) / 1_000_000, \"M\")\n",
    "print(\"Number of param (D):\", sum(p.numel() for p in D.parameters()) / 1_000_000, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Resize((128, 128), antialias=None), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import PairWiseImages, PairWiseImagesRGBE\n",
    "\n",
    "### RGBE format\n",
    "if config[\"RGBE\"]:\n",
    "    pair = PairWiseImagesRGBE(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                        \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                        transform=train_transform, device=device)\n",
    "    \n",
    "### Original dataset RGB format\n",
    "else:\n",
    "    pair = PairWiseImages(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                        \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                        transform=train_transform, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the whole dataset\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "if config[\"nb_images\"] > len(pair):\n",
    "    raise ValueError(\"Number of images to train is greater than the dataset size\")\n",
    "\n",
    "elif config[\"nb_images\"] == -1 or config[\"nb_images\"] == len(pair):\n",
    "    print(\"Training on the whole dataset\")\n",
    "    pair_subset = pair\n",
    "else:\n",
    "    print(\"Training on a subset of the dataset\")\n",
    "    indices = torch.arange(config[\"nb_images\"])\n",
    "    pair_subset = Subset(pair, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "length = len(pair_subset)\n",
    "test_length = int(0.2 * length)\n",
    "\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "train_data, valid_data = torch.utils.data.random_split(pair_subset, [length - test_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 5)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_ckp(checkpoint, checkpoint_dir, suffix=\"\"):\n",
    "    old_path = f\"{checkpoint_dir}/checkpoint_{suffix}_{checkpoint['epoch']-1}.pt\"\n",
    "    if os.path.exists(old_path):\n",
    "        os.remove(old_path)\n",
    "    f_path = f\"{checkpoint_dir}/checkpoint_{suffix}_{checkpoint['epoch']}.pt\"\n",
    "    torch.save(checkpoint, f_path)\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moskarjor\u001b[0m (\u001b[33mdeldrify\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cde4eca29a42cea954dde589fb0956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167396300000595, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/oskarjor/DeLDRify/wandb/run-20231203_145717-4epck0q8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deldrify/DeLDRify/runs/4epck0q8' target=\"_blank\">smooth-galaxy-48</a></strong> to <a href='https://wandb.ai/deldrify/DeLDRify' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deldrify/DeLDRify' target=\"_blank\">https://wandb.ai/deldrify/DeLDRify</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deldrify/DeLDRify/runs/4epck0q8' target=\"_blank\">https://wandb.ai/deldrify/DeLDRify/runs/4epck0q8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "                    project=\"DeLDRify\",\n",
    "                    config=config,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_pixel = torch.nn.L1Loss().to(device)\n",
    "criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cache-smooth-galaxy-48'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir = './cache-' + run.name\n",
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/Users/oskarjor/miniconda3/envs/hdrgan/lib/python3.9/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 52%|█████▏    | 207/400 [1:54:20<1:22:48, 25.74s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import RGBE_to_RGB, preprocess_tensor_to_array, hdr_np_to_img_manual, hdr_np_to_img_tonemap\n",
    "\n",
    "retrain = False\n",
    "d_path = \"cache-peachy-dawn-11/checkpoint_discriminator_20.pt\"\n",
    "g_path = \"cache-peachy-dawn-11/checkpoint_generator_20.pt\"\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "\n",
    "loss_scaling_factor = config[\"loss_scaling_factor\"]\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "\n",
    "import json\n",
    "with open(results_dir + '/metadata.json', 'w') as fp:\n",
    "    json.dump(config, fp)\n",
    "\n",
    "\n",
    "if retrain:\n",
    "    G, optimizer_G, start_epoch = load_ckp(g_path, G, optimizer_G)\n",
    "    D, optimizer_D, start_epoch = load_ckp(d_path, D, optimizer_D)\n",
    "    epochs = epochs + start_epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(start_epoch,epochs)):\n",
    "    total_loss_G = 0\n",
    "    total_loss_D = 0\n",
    "    total_loss_D_real = 0\n",
    "    total_loss_D_fake = 0\n",
    "\n",
    "    for ldr, hdr in tqdm(train_dataloader, leave=False):\n",
    "\n",
    "        D_output_shape = D.out_channels\n",
    "\n",
    "        valid = torch.tensor(np.ones((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "        fake = torch.tensor(np.zeros((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_hdr = G(ldr)\n",
    "\n",
    "        loss_pixel = criterion_pixel(fake_hdr, hdr)\n",
    "\n",
    "        pred_real = D(hdr).detach()\n",
    "        pred_fake = D(fake_hdr)\n",
    "\n",
    "        loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        loss_G = loss_pixel + loss_scaling_factor * loss_GAN\n",
    "        total_loss_G += loss_G.item()\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = D(hdr)\n",
    "        pred_fake = D(fake_hdr.detach())\n",
    "\n",
    "        loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        total_loss_D_real += loss_real.item()\n",
    "        total_loss_D_fake += loss_fake.item()\n",
    "        total_loss_D += loss_D.item()\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    valid_total_loss_G = 0\n",
    "    valid_total_loss_D = 0\n",
    "    valid_total_loss_D_real = 0\n",
    "    valid_total_loss_D_fake = 0\n",
    "\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    with torch.no_grad():\n",
    "        for ldr_valid, hdr_valid in tqdm(valid_dataloader, leave=False):\n",
    "            D_output_shape = D.out_channels\n",
    "\n",
    "            valid = torch.tensor(np.ones((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "            fake = torch.tensor(np.zeros((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "\n",
    "            # Validate Generator\n",
    "            fake_hdr = G(ldr)\n",
    "\n",
    "            loss_pixel = criterion_pixel(fake_hdr, hdr)\n",
    "\n",
    "            pred_real = D(hdr).detach()\n",
    "            pred_fake = D(fake_hdr)\n",
    "\n",
    "            loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "            loss_G = loss_pixel + loss_scaling_factor * loss_GAN\n",
    "            valid_total_loss_G += loss_G.item()\n",
    "\n",
    "            # Validate Discriminator\n",
    "            pred_real = D(hdr)\n",
    "            pred_fake = D(fake_hdr.detach())\n",
    "\n",
    "            loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "            loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "            valid_total_loss_D_real += loss_real.item()\n",
    "            valid_total_loss_D_fake += loss_fake.item()\n",
    "            valid_total_loss_D += loss_D.item()\n",
    "\n",
    "    wandb.log({\n",
    "        \"eval/loss_G\": valid_total_loss_G / len(valid_dataloader), \n",
    "        \"eval/loss_D\": valid_total_loss_D / len(valid_dataloader), \n",
    "        \"eval/loss_D_real\": valid_total_loss_D_real / len(valid_dataloader), \n",
    "        \"eval/loss_D_fake\": valid_total_loss_D_fake / len(valid_dataloader), \n",
    "        \"train/loss_G\": total_loss_G / len(train_dataloader), \n",
    "        \"train/loss_D\": total_loss_D / len(train_dataloader), \n",
    "        \"train/loss_D_real\": total_loss_D_real / len(train_dataloader), \n",
    "        \"train/loss_D_fake\": total_loss_D_fake / len(train_dataloader), \n",
    "        \"epoch\": epoch+1\n",
    "        })\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    checkpoint_G = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': G.state_dict(),\n",
    "        'optimizer': optimizer_G.state_dict(),\n",
    "    }\n",
    "\n",
    "    checkpoint_D = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': D.state_dict(),\n",
    "        'optimizer': optimizer_D.state_dict(),\n",
    "    }\n",
    "\n",
    "    save_ckp(checkpoint_G,results_dir,\"generator\")\n",
    "    save_ckp(checkpoint_D,results_dir,\"discriminator\")\n",
    "\n",
    "ldr, hdr = next(iter(valid_dataloader))\n",
    "\n",
    "real_hdr = hdr\n",
    "fake_hdr = G(ldr)\n",
    "\n",
    "IMG_INDEX = 1\n",
    "\n",
    "fake_arr = preprocess_tensor_to_array(fake_hdr[IMG_INDEX].detach(), RGBE=config[\"RGBE\"])\n",
    "if config[\"RGBE\"]:\n",
    "    fake_arr_RGB = RGBE_to_RGB(fake_arr)\n",
    "    fake_hdr_indices = fake_arr_RGB > 1\n",
    "    fake_img = hdr_np_to_img_tonemap(fake_arr_RGB)\n",
    "else:\n",
    "    fake_img = hdr_np_to_img_tonemap(fake_arr)\n",
    "\n",
    "real_arr = preprocess_tensor_to_array(real_hdr[IMG_INDEX].detach(), RGBE=config[\"RGBE\"])\n",
    "if config[\"RGBE\"]:\n",
    "    real_arr_RGB = RGBE_to_RGB(real_arr)\n",
    "    real_img = hdr_np_to_img_tonemap(real_arr_RGB)\n",
    "else:\n",
    "    real_img = hdr_np_to_img_tonemap(real_arr)\n",
    "\n",
    "real_image = wandb.Image(real_img, caption=f\"Real HDR\")\n",
    "fake_image = wandb.Image(fake_img, caption=f\"Fake HDR\")\n",
    "\n",
    "wandb.log({\"real_hdr\": real_image, \"fake_hdr\": fake_image})\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
