{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeLDRify - ESRGAN applied to single-track LDR to HDR image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and initalize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"RGBE\": False,\n",
    "            \"in_nc\":3,\n",
    "            \"nf\":64,\n",
    "            \"nb\":4,\n",
    "            \"gc\":32,\n",
    "            \"channels\": 8,\n",
    "            \"nb_images\":-1,\n",
    "            \"batch_size\":8,\n",
    "            \"epochs\":400,\n",
    "            \"loss_scaling_factor\":0.03,\n",
    "            \"depth\": 2, \n",
    "        }\n",
    "\n",
    "if config[\"RGBE\"]:\n",
    "    config[\"in_channels\"] = 4\n",
    "    config[\"out_nc\"] = 4\n",
    "    config[\"out_channels\"] = 4\n",
    "else:\n",
    "    config[\"in_channels\"] = 3\n",
    "    config[\"out_nc\"] = 3\n",
    "    config[\"out_channels\"] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RRDBNet\n",
    "from discriminator import DiscriminatorForVGG, DiscriminatorForVGG2\n",
    "\n",
    "G = RRDBNet(in_nc=config[\"in_nc\"], out_nc=config[\"out_nc\"], nf=config[\"nf\"], nb=config[\"nb\"], gc=config[\"gc\"])\n",
    "D = DiscriminatorForVGG2(in_channels=config[\"in_channels\"], out_channels=config[\"out_channels\"], channels=config[\"channels\"], depth=config[\"depth\"])\n",
    "\n",
    "G.to(device=device)\n",
    "D.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param (G): 2.918147 M\n",
      "Number of param (D): 0.258627 M\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of param (G):\", sum(p.numel() for p in G.parameters()) / 1_000_000, \"M\")\n",
    "print(\"Number of param (D):\", sum(p.numel() for p in D.parameters()) / 1_000_000, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Resize((128, 128), antialias=None), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import PairWiseImages, PairWiseImagesRGBE\n",
    "\n",
    "### RGBE format\n",
    "if config[\"RGBE\"]:\n",
    "    pair = PairWiseImagesRGBE(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                        \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                        transform=train_transform, device=device)\n",
    "    \n",
    "### Original dataset RGB format\n",
    "else:\n",
    "    pair = PairWiseImages(\"LDR-HDR-pair_Dataset-master/LDR_exposure_0/\", \n",
    "                        \"LDR-HDR-pair_Dataset-master/HDR/\", \n",
    "                        transform=train_transform, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the whole dataset\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "if config[\"nb_images\"] > len(pair):\n",
    "    raise ValueError(\"Number of images to train is greater than the dataset size\")\n",
    "\n",
    "elif config[\"nb_images\"] == -1 or config[\"nb_images\"] == len(pair):\n",
    "    print(\"Training on the whole dataset\")\n",
    "    pair_subset = pair\n",
    "else:\n",
    "    print(\"Training on a subset of the dataset\")\n",
    "    indices = torch.arange(config[\"nb_images\"])\n",
    "    pair_subset = Subset(pair, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "length = len(pair_subset)\n",
    "test_length = int(0.2 * length)\n",
    "\n",
    "train_data, valid_data = torch.utils.data.random_split(pair_subset, [length - test_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_ckp(checkpoint, checkpoint_dir, suffix=\"\"):\n",
    "    old_path = f\"{checkpoint_dir}/checkpoint_{suffix}_{checkpoint['epoch']-1}.pt\"\n",
    "    if os.path.exists(old_path):\n",
    "        os.remove(old_path)\n",
    "    f_path = f\"{checkpoint_dir}/checkpoint_{suffix}_{checkpoint['epoch']}.pt\"\n",
    "    torch.save(checkpoint, f_path)\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moskarjor\u001b[0m (\u001b[33mdeldrify\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/oskarjor/DeLDRify/wandb/run-20231203_115709-9e1tcy7e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deldrify/DeLDRify/runs/9e1tcy7e' target=\"_blank\">decent-vortex-46</a></strong> to <a href='https://wandb.ai/deldrify/DeLDRify' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deldrify/DeLDRify' target=\"_blank\">https://wandb.ai/deldrify/DeLDRify</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deldrify/DeLDRify/runs/9e1tcy7e' target=\"_blank\">https://wandb.ai/deldrify/DeLDRify/runs/9e1tcy7e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "                    project=\"DeLDRify\",\n",
    "                    config=config,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_pixel = torch.nn.L1Loss().to(device)\n",
    "criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cache-decent-vortex-46'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir = './cache-' + run.name\n",
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/Users/oskarjor/miniconda3/envs/hdrgan/lib/python3.9/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 56%|█████▋    | 225/400 [1:23:50<1:05:02, 22.30s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import RGBE_to_RGB, preprocess_tensor_to_array, hdr_np_to_img_manual, hdr_np_to_img_tonemap\n",
    "\n",
    "retrain = False\n",
    "d_path = \"cache-peachy-dawn-11/checkpoint_discriminator_20.pt\"\n",
    "g_path = \"cache-peachy-dawn-11/checkpoint_generator_20.pt\"\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "\n",
    "loss_scaling_factor = config[\"loss_scaling_factor\"]\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "\n",
    "import json\n",
    "with open(results_dir + '/metadata.json', 'w') as fp:\n",
    "    json.dump(config, fp)\n",
    "\n",
    "\n",
    "if retrain:\n",
    "    G, optimizer_G, start_epoch = load_ckp(g_path, G, optimizer_G)\n",
    "    D, optimizer_D, start_epoch = load_ckp(d_path, D, optimizer_D)\n",
    "    epochs = epochs + start_epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(start_epoch,epochs)):\n",
    "    total_loss_G = 0\n",
    "    total_loss_D = 0\n",
    "    total_loss_D_real = 0\n",
    "    total_loss_D_fake = 0\n",
    "\n",
    "    for ldr, hdr in tqdm(train_dataloader, leave=False):\n",
    "\n",
    "        D_output_shape = D.out_channels\n",
    "\n",
    "        valid = torch.tensor(np.ones((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "        fake = torch.tensor(np.zeros((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_hdr = G(ldr)\n",
    "\n",
    "        loss_pixel = criterion_pixel(fake_hdr, hdr)\n",
    "\n",
    "        pred_real = D(hdr).detach()\n",
    "        pred_fake = D(fake_hdr)\n",
    "\n",
    "        loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        loss_G = loss_pixel + loss_scaling_factor * loss_GAN\n",
    "        total_loss_G += loss_G.item()\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = D(hdr)\n",
    "        pred_fake = D(fake_hdr.detach())\n",
    "\n",
    "        loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        total_loss_D_real += loss_real.item()\n",
    "        total_loss_D_fake += loss_fake.item()\n",
    "        total_loss_D += loss_D.item()\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    valid_total_loss_G = 0\n",
    "    valid_total_loss_D = 0\n",
    "    valid_total_loss_D_real = 0\n",
    "    valid_total_loss_D_fake = 0\n",
    "\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    with torch.no_grad():\n",
    "        for ldr_valid, hdr_valid in tqdm(valid_dataloader, leave=False):\n",
    "            D_output_shape = D.out_channels\n",
    "\n",
    "            valid = torch.tensor(np.ones((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "            fake = torch.tensor(np.zeros((ldr.size(0), D_output_shape)), requires_grad=False, dtype=torch.float32).to(device=device)\n",
    "\n",
    "            # Validate Generator\n",
    "            fake_hdr = G(ldr)\n",
    "\n",
    "            loss_pixel = criterion_pixel(fake_hdr, hdr)\n",
    "\n",
    "            pred_real = D(hdr).detach()\n",
    "            pred_fake = D(fake_hdr)\n",
    "\n",
    "            loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "            loss_G = loss_pixel + loss_scaling_factor * loss_GAN\n",
    "            valid_total_loss_G += loss_G.item()\n",
    "\n",
    "            # Validate Discriminator\n",
    "            pred_real = D(hdr)\n",
    "            pred_fake = D(fake_hdr.detach())\n",
    "\n",
    "            loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "            loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "            valid_total_loss_D_real += loss_real.item()\n",
    "            valid_total_loss_D_fake += loss_fake.item()\n",
    "            valid_total_loss_D += loss_D.item()\n",
    "\n",
    "    wandb.log({\n",
    "        \"eval/loss_G\": valid_total_loss_G / len(valid_dataloader), \n",
    "        \"eval/loss_D\": valid_total_loss_D / len(valid_dataloader), \n",
    "        \"eval/loss_D_real\": valid_total_loss_D_real / len(valid_dataloader), \n",
    "        \"eval/loss_D_fake\": valid_total_loss_D_fake / len(valid_dataloader), \n",
    "        \"train/loss_G\": total_loss_G / len(train_dataloader), \n",
    "        \"train/loss_D\": total_loss_D / len(train_dataloader), \n",
    "        \"train/loss_D_real\": total_loss_D_real / len(train_dataloader), \n",
    "        \"train/loss_D_fake\": total_loss_D_fake / len(train_dataloader), \n",
    "        \"epoch\": epoch+1\n",
    "        })\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    checkpoint_G = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': G.state_dict(),\n",
    "        'optimizer': optimizer_G.state_dict(),\n",
    "    }\n",
    "\n",
    "    checkpoint_D = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': D.state_dict(),\n",
    "        'optimizer': optimizer_D.state_dict(),\n",
    "    }\n",
    "\n",
    "    save_ckp(checkpoint_G,results_dir,\"generator\")\n",
    "    save_ckp(checkpoint_D,results_dir,\"discriminator\")\n",
    "\n",
    "ldr, hdr = next(iter(valid_dataloader))\n",
    "\n",
    "real_hdr = hdr\n",
    "fake_hdr = G(ldr)\n",
    "\n",
    "IMG_INDEX = 1\n",
    "\n",
    "fake_arr = preprocess_tensor_to_array(fake_hdr[IMG_INDEX].detach(), RGBE=config[\"RGBE\"])\n",
    "if config[\"RGBE\"]:\n",
    "    fake_arr_RGB = RGBE_to_RGB(fake_arr)\n",
    "    fake_hdr_indices = fake_arr_RGB > 1\n",
    "    fake_img = hdr_np_to_img_tonemap(fake_arr_RGB)\n",
    "else:\n",
    "    fake_img = hdr_np_to_img_tonemap(fake_arr)\n",
    "\n",
    "real_arr = preprocess_tensor_to_array(real_hdr[IMG_INDEX].detach(), RGBE=config[\"RGBE\"])\n",
    "if config[\"RGBE\"]:\n",
    "    real_arr_RGB = RGBE_to_RGB(real_arr)\n",
    "    real_img = hdr_np_to_img_tonemap(real_arr_RGB)\n",
    "else:\n",
    "    real_img = hdr_np_to_img_tonemap(real_arr)\n",
    "\n",
    "real_image = wandb.Image(real_img, caption=f\"Real HDR\")\n",
    "fake_image = wandb.Image(fake_img, caption=f\"Fake HDR\")\n",
    "\n",
    "wandb.log({\"real_hdr\": real_image, \"fake_hdr\": fake_image})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RGBE_to_RGB, preprocess_tensor_to_array\n",
    "from utils import hdr_np_to_img_manual, hdr_np_to_img_tonemap, np_to_img_naive\n",
    "\n",
    "\n",
    "\n",
    "# load data from train dataset onto cpu\n",
    "ldr, hdr = train_data[0]\n",
    "ldr = preprocess_tensor_to_array(ldr, RGBE=False)\n",
    "hdr = preprocess_tensor_to_array(hdr, RGBE=config[\"RGBE\"])\n",
    "\n",
    "# convert from RGBE to RGB\n",
    "if config[\"RGBE\"]:\n",
    "    hdr = RGBE_to_RGB(hdr.copy())\n",
    "\n",
    "# create PIL images (naive approach)\n",
    "ldr_img = np_to_img_naive(ldr)\n",
    "hdr_img = np_to_img_naive(hdr)\n",
    "\n",
    "# create PIL images (tonemap approach)\n",
    "new_hdr_img = hdr_np_to_img_manual(hdr)\n",
    "new_hdr2_img = hdr_np_to_img_tonemap(hdr)\n",
    "\n",
    "# list of images to display\n",
    "images = [ldr_img, hdr_img, new_hdr_img, new_hdr2_img]\n",
    "\n",
    "# display images\n",
    "display(*images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RRDBNet\n",
    "from discriminator import DiscriminatorForVGG\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# cache to read model weights from\n",
    "target_dir = \"cache-feasible-feather-20\"\n",
    "config_path = target_dir + \"/metadata.json\"\n",
    "with open(config_path) as conf_file:\n",
    "    config = json.load(conf_file)\n",
    "\n",
    "# create models\n",
    "G = RRDBNet(in_nc=config[\"in_nc\"], out_nc=config[\"out_nc\"], nf=config[\"nf\"], nb=config[\"nb\"], gc=config[\"gc\"])\n",
    "D = DiscriminatorForVGG(in_channels=config[\"in_channels\"], out_channels=config[\"out_channels\"], channels=config[\"channels\"])\n",
    "\n",
    "# load weights\n",
    "G, _, _ = load_ckp(target_dir + \"/checkpoint_generator_400.pt\", G, None)\n",
    "D, _, _ = load_ckp(target_dir + \"/checkpoint_discriminator_400.pt\", D, None)\n",
    "\n",
    "# move models to device (cpu, cuda or mps)\n",
    "G.to(device=device)\n",
    "D.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a batch from the validation dataset\n",
    "ldr, hdr = next(iter(valid_dataloader))\n",
    "\n",
    "# create a real, generated and random HDR image\n",
    "real_hdr = hdr\n",
    "fake_hdr = G(ldr)\n",
    "random_hdr = torch.rand_like(hdr)\n",
    "\n",
    "# run discriminator on the images\n",
    "print(\"Real HDR images (values should be positive)\")\n",
    "print(D(hdr).mean(0, keepdim=True).detach())\n",
    "print()\n",
    "\n",
    "print(\"Random HDR images (values should be negative)\")\n",
    "print(D(random_hdr).mean(0, keepdim=True).detach())\n",
    "print()\n",
    "\n",
    "print(\"Generated HDR images (values should be positive for a good generator, and negative for a good discriminator)\")\n",
    "print(D(fake_hdr).mean(0, keepdim=True).detach())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RGBE_to_RGB, preprocess_tensor_to_array, hdr_np_to_img_manual, hdr_np_to_img_tonemap\n",
    "\n",
    "IMG_INDEX = 1\n",
    "\n",
    "fake_arr = preprocess_tensor_to_array(fake_hdr[IMG_INDEX].detach(), RGBE=config[\"RGBE\"])\n",
    "if config[\"RGBE\"]:\n",
    "    fake_arr_RGB = RGBE_to_RGB(fake_arr)\n",
    "    fake_hdr_indices = fake_arr_RGB > 1\n",
    "    fake_img = hdr_np_to_img_tonemap(fake_arr_RGB)\n",
    "else:\n",
    "    fake_img = hdr_np_to_img_tonemap(fake_arr)\n",
    "\n",
    "real_arr = preprocess_tensor_to_array(real_hdr[IMG_INDEX].detach(), RGBE=config[\"RGBE\"])\n",
    "if config[\"RGBE\"]:\n",
    "    real_arr_RGB = RGBE_to_RGB(real_arr)\n",
    "    real_img = hdr_np_to_img_tonemap(real_arr_RGB)\n",
    "else:\n",
    "    real_img = hdr_np_to_img_tonemap(real_arr)\n",
    "\n",
    "display(fake_img, real_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_hdr_indices = real_arr_RGB > 1\n",
    "fake_hdr_indices = fake_arr_RGB > 1\n",
    "\n",
    "np.count_nonzero(1 - real_hdr_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "np_real_img = np.array(real_img)\n",
    "R = np_real_img[:, :, 0]\n",
    "G = np_real_img[:, :, 1]\n",
    "B = np_real_img[:, :, 2]\n",
    "np_real_img[real_arr_RGB < 1] = 255\n",
    "real_img_filtered = PIL.Image.fromarray(np_real_img)\n",
    "real_img_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np_real_img, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "cv.imwrite(\"fake_img.hdr\", np.array(fake_img))  # save image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assert that RGB -> RGBE -> RGB conversion is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from utils import RGBE_to_RGB, RGB_to_RGBE\n",
    "\n",
    "np_hdr = cv.imread(\"LDR-HDR-pair_Dataset-master/HDR/HDR_001.hdr\", flags=cv.IMREAD_ANYDEPTH)\n",
    "print(np_hdr.shape, np_hdr.dtype, np_hdr.max(), np_hdr.min(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_hdr = RGB_to_RGBE(np_hdr)\n",
    "print(converted_hdr.shape, converted_hdr.dtype, converted_hdr.max(), converted_hdr.min(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_hdr_exp = converted_hdr[:, :, 3]\n",
    "converted_hdr_ch = converted_hdr[:, :, :3]\n",
    "print(converted_hdr_exp.max(), converted_hdr_exp.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_converted_hdr = RGBE_to_RGB(converted_hdr)\n",
    "print(double_converted_hdr.shape, double_converted_hdr.dtype, double_converted_hdr.max(), double_converted_hdr.min(), sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
